{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyP8hGeNXiil0YG5jMIlqaKl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Initial Environment - Long Only"],"metadata":{"id":"kBN3mJh8GIZK"}},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":240},"id":"r_uE88ytGHGF","executionInfo":{"status":"error","timestamp":1684451038272,"user_tz":-60,"elapsed":247,"user":{"displayName":"Yen Hann","userId":"05398580559176166502"}},"outputId":"a59dc4eb-1263-463d-973a-01ce81c2e8d2"},"outputs":[{"output_type":"error","ename":"NameError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-171d809bb253>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mLongOnlyEnv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEnv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'render.modes'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     def __init__(self, \n","\u001b[0;31mNameError\u001b[0m: name 'gym' is not defined"]}],"source":["class LongOnlyEnv(gym.Env):\n","\n","    metadata = {'render.modes': ['human']}\n","\n","    def __init__(self, \n","                df,\n","                stock_dim=17,\n","                initial_amount=1e4,\n","                state_space=13,\n","                action_space=17,\n","                lookback=13,\n","                max_timesteps=1e4,\n","                mode='train'):\n","\n","        # Initialise parameters\n","        self.t = 0                                                                    # Initialise initial timestep, t, to be 0\n","        self.max_timesteps = max_timesteps                                            # Maximum number of timesteps per episode/trajectory\n","        self.lookback = lookback\n","        self.df = df\n","        self.stock_dim = stock_dim                                                    # 17 different assets\n","        self.initial_amount = initial_amount                                          # Initial investment amount\n","        self.state_space = state_space                                                # 16 different features (14x action-independent, 2x action-dependent)\n","        self.action_space = action_space                                              # 17 continuous actions (weights)\n","        self.current_episode = 0                                                      # Episode counter\n","        self.mode = mode                                                              # Mode = 'train' prints episodic items, mode = 'predict' doesn't\n","\n","        # Action_space normalization and shape is self.stock_dim\n","        self.action_space = spaces.Box(low = 0, high = 1,shape = (self.action_space,))                                                              # The action space outputted should be a 1D 16-dimensional array (assuming 16 stocks)\n","        \n","        # State space shape = (17, 16)\n","        # We have 17 unique cryptocurrencies and 16 features\n","        # For each crypto we have: 1x closing price at t, 12x percentage changes (t-11, t-10, ... , t), 1x EMA_t, 1x current holdings, 1x invested amount\n","        # No time dimesnion, each observation space is representative of a given timestep\n","        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(self.stock_dim, self.state_space))                                     # The observation space (14x action independent features, 2x action dependent features)\n","\n","        # ------------------ Load data from a pandas dataframe -----------------------------\n","        ## Extracts the data for the current timestep (specified by self.t) from the input DataFrame self.df. This data includes close prices, % changes, and EMA\n","        self.data = self.df[self.t]\n","\n","        ## Constructs the state array by combining the action independent features: closing price t, 12x percentage change in closing prices, EMA_t\n","        ## The resulting self.state is a 2D array with the shape (17, 13), as discussed earlier\n","        self.state = self.data\n","\n","        ## Initialise terminality. False at the beginning. Will be updated as we train\n","        self.terminal = False    \n","        \n","        # ------------------ Initialises the agent's portfolio value to the initial investment amount --------------------------------\n","        ## This portfolio value will be updated as we go along\n","        self.portfolio_value = self.initial_amount\n","\n","        # Memorize portfolio value each step\n","        ## Initialises a list called `self.asset_memory` to store the agent's portfolio value at each time step. The list starts with the initial investment amount.\n","        self.asset_memory = [self.initial_amount]\n","\n","        # Memorize portfolio return each step\n","        ## Initialises a list called `self.portfolio_return_memory` to store the agent's daily portfolio return at each time step. The list starts with 0, as there is no return on the first day.\n","        self.portfolio_return_memory = [0]\n","\n","        ## Initialises a list called `self.actions_memory` to store the agent's actions (portfolio weights) at each time step. The list starts with equal weights for all stocks in the portfolio (i.e., 1 divided by the number of stocks).\n","        self.actions_memory = [[0] * (self.stock_dim-1) + [1]]                    # 16 crypto assets, 1 cash asset. Initialise by allocating all funds to the cash asset\n","\n","        ## Initialises a list called self.date_memory to store each time step. The list starts with the timestep of the first data entry in the environment's data (self.data).\n","        self.timestep_memory = [self.t]\n","\n","        \n","    def step(self, actions):\n","        ## Checks if the current t >= max_timesteps. If true, sets self.terminal to True, indicating the end of the episode.\n","        self.terminal = self.t >= self.max_timesteps - 1\n","        \n","        # ------------------ If end of episode (i.e., the last timestep, which is user specified) -----------------\n","        if self.terminal:\n","\n","          cumulative_return = (self.portfolio_value / self.initial_amount) - 1\n","\n","          if self.mode == 'train':\n","            print(f\"Episode: {self.current_episode} | Timesteps: {self.t + 1} | Cumulative Return: {cumulative_return:.2%}\")\n","            \n","          ## Finally, it returns the current state, reward, and terminal status along with an empty dictionary.\n","          return self.state, self.reward, self.terminal, {}\n","\n","        # ----------------- If the current timestep is not the last timestep ---------------------\n","        else:\n","            \n","            ## Normalize the input actions using softmax normalization, which ensures that the portfolio weights sum to 1.\n","            weights = self.softmax_normalization(torch.tensor(actions))\n","\n","            ## Append the normalized weights to the actions memory.\n","            self.actions_memory.append(weights)\n","\n","            ## Store the data for the current timestep before moving to the next timestep.\n","            prev_timestep_memory = self.data\n","\n","            # ------------------------ Next State ------------------------------\n","            ##  Increment the `time` counter to move to the next timestep.\n","            self.t += 1\n","            \n","            ## Update `self.data` and `self.state` to the new timestep's values\n","            ## Creating a new state with the new timestep action independent features and create new action dependent features\n","            self.data = self.df[self.t]\n","\n","            self.state = self.data\n","\n","            # Calculate portfolio return\n","            ## Portfolio return (a scalar) based on the new timestep's closing prices and the portfolio weights (returns x weights)\n","            ## [close_price_t / close_price_(t-1)] - 1. If positive, then returns are profitable, otherwise loss\n","            ## NOTE: (self.data.close.values / prev_timestep_memory.close.values) - 1 : Calculates the percentage gain/loss for each crypto\n","            ## NOTE: Multiplying these percentage returns by their respective weights (*weights) gives the weighted return of each crypto\n","            ## NOTE: Then sum over all stocks to calculate the portfolio return for that timestep\n","            ## Column 0 = closing prices. Row index = cryptocurrencies\n","            portfolio_return = torch.sum(((self.data[:,0] / prev_timestep_memory[:,0]) - 1) * weights)\n","\n","            # Update portfolio value\n","            ## Calculate the new portfolio value based on the calculated portfolio return from before\n","            new_portfolio_value = self.portfolio_value*(1+portfolio_return)\n","\n","            ## Update the portfolio value with the new value\n","            self.portfolio_value = new_portfolio_value\n","\n","            # Save into memory: the new portfolio return, new date, and new portfolio value\n","            self.portfolio_return_memory.append(portfolio_return)\n","            self.timestep_memory.append(self.t)            \n","            self.asset_memory.append(new_portfolio_value)\n","\n","            # # The reward is the immediate reward at the current timestep not the cumulative returns (i.e., new_portfolio_value)\n","            self.reward = portfolio_return\n","\n","        # Return the updated state, reward, terminal status, and an empty dictionary\n","        return self.state, self.reward, self.terminal, {}\n","\n","    \n","    # The environment is reset to its initial state, which is necessary for the beginning of a new training episode or evaluation\n","    def reset(self):\n","        \n","        ## Resets the asset_memory to the initial amount (i.e., portfolio value)\n","        self.asset_memory = [self.initial_amount]\n","\n","        ## Resets the timestep counter to 0 (the first timestep)\n","        self.t = 0\n","\n","        ## Increment episode number by 1\n","        self.current_episode += 1\n","\n","        ## Retrieves the data for the first timestep from the dataframe\n","        self.data = self.df[self.t]\n","        \n","        self.state = self.data\n","\n","        ## Resets the portfolio value to the initial amount\n","        self.portfolio_value = self.initial_amount\n","\n","        ## Resets the terminal flag to False, indicating that the episode has not ended\n","        self.terminal = False \n","\n","        ## Resets the portfolio_return_memory to its initial state, with a 0% return (no return on the first timestep)\n","        self.portfolio_return_memory = [0]\n","\n","        ## Resets the actions_memory with an initial equal-weighted portfolio\n","        self.actions_memory = [[0] * (self.stock_dim-1) + [1]]                    # 16 crypto assets, 1 cash asset. Initialise by allocating all funds to the cash asset\n","\n","        ## Resets the timestep_memory with the first timestep\n","        self.timestep_memory = [self.t]\n","\n","        # Initial state is returned \n","        return self.state\n","    \n","    #  In general, the render method is used to display the current state of the environment, and the mode argument can be used to determine how this visualisation should be presented\n","    ## Typically, a mode of 'human' would mean that the visualisation is intended to be easily interpretable by humans, often with some kind of graphical display\n","    ## In this specific implementation, the render method does not provide any visualisation, regardless of the mode being set to 'human'\n","    ## Hence, the method simply returns the current state of the environment.\n","    def render(self, mode='human'):\n","        return self.state\n","    \n","    # Softmax_normalization method, which takes actions as input and returns normalized actions using the softmax function.\n","    def softmax_normalization(self, actions):\n","\n","        ## Calculates the exponential of each action in the input list\n","        numerator = torch.exp(torch.tensor(actions))\n","\n","        ## Calculates the sum of the exponentials of the actions\n","        denominator = torch.sum(np.exp(torch.tensor(actions)))\n","        \n","        ## Divides each exponential action by the sum of the exponentials to normalize the actions\n","        softmax_output = numerator/denominator\n","\n","        ## Returns the normalized actions\n","        return softmax_output\n","\n","    # Saves the date and daily return of the portfolio into a DataFrame and return this DataFrame\n","    def save_asset_memory(self):\n","\n","        # ## Retrieves the date_memory list (all dates in an episode)\n","        # timestep_list = self.timestep_memory\n","\n","        # ## Retrieves the portfolio_return_memory list (all returns in an episode)\n","        # portfolio_return = self.portfolio_return_memory\n","\n","        # ## Creates a DataFrame with 2 columns: 'date' with the dates from date_memory\n","        # ## 'daily_return' with the portfolio returns from portfolio_return_memory.\n","        # df_account_value = pd.DataFrame({'timestep':timestep_list,'daily_return':portfolio_return})\n","\n","        # ## Returns the DataFrame containing the date and daily return of the portfolio\n","        # ## Each row in the DataFrame is basically an episode/trajectory\n","        # return df_account_value\n","        return self.asset_memory\n","\n","    def save_action_memory(self):\n","\n","        ## Retrieves the date_memory list (all dates in an episode)\n","        timestep_list = self.timestep_memory\n","\n","        ## Creates a DataFrame using the list of dates in an episode and labels the column as 'date'\n","        ## Basically a DataFrame with 1 column = 'date'\n","        df_timestep = pd.DataFrame(timestep_list)\n","        df_timestep.columns = ['timestep']\n","        \n","        ## Retrieves the list of actions (i.e., a list of 17 weights for a given time step) taken at each time step of an episode\n","        ## Hence, our action_list is basically n_timesteps x [a list of 17 weights] (i.e., n_timestep lists)\n","        action_list = self.actions_memory\n","\n","        ## Creates a DataFrame using the list of actions with column names set as the stock ticker symbols (AAPL, FB, GOOGL, etc.)\n","        ## The dimension of df_actions is (n_dates, n_stocks), where n_dates is the number of timesteps in any episode (should be constant) and n_stocks is the number of stocks in the portfolio\n","        ## Each row in `df_actions` corresponds to a given date, and the value in a given row will be the portfolio weights of each stock (basically unpacked the list inside the list)\n","        df_actions = pd.DataFrame(action_list)\n","        df_actions.columns = ['ADA', 'ATOM', 'AVAX', 'BNB', 'BTC', 'DOGE', 'DOT', 'ETH', 'LINK', 'LTC', 'MATIC', 'SHIB', 'SOL', 'TRX', 'UNI', 'XRP', 'USD']\n","        df_actions.index = df_timestep.timestep\n","\n","        # Return the generated action DataFrame\n","        return df_actions\n","\n","    # Sets the random seed for the environment\n","    ## If a seed value is passed, it uses that value, otherwise it generates a new random seed using seeding.np_random(seed)\n","    ## It returns a list containing the generated seed value.\n","    def _seed(self, seed=None):\n","        self.np_random, seed = seeding.np_random(seed)\n","        return [seed]\n","    \n","    # Helper function that returns a stable-baselines compatible vectorized environment (e) and the initial observation of the environment (obs)\n","    ## GOAL: To integrate the environment with Stable Baselines (basically translating our user defined environment to make it compatible with Stable Baselines)\n","    def get_sb_env(self):\n","\n","        ## Creates a DummyVecEnv instance (e) with a lambda function that returns the environment itself. \n","        e = DummyVecEnv([lambda: self])\n","\n","        ## Calls the reset method of the DummyVecEnv to get the initial observation\n","        obs = e.reset()\n","\n","        ## Returns the DummyVecEnv instance (e) and the initial observation\n","        return e, obs"]}]}